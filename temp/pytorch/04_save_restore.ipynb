{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Restoring Models\n",
    "\n",
    "* Once you train your models, you may save it and reload it later.\n",
    "* You may save just the weights or the whole architecture \n",
    "* Saving the model including the architecture uses bit more space but well worth it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import standard libraries that you may use most times\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from torchinfo import summary\n",
    "from torchvision import transforms, datasets\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "# Bring back MNIST dataset\n",
    "\n",
    "# Convert Pil image to PyTorch Tensor\n",
    "data_transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the data set\n",
    "mnist_train = datasets.MNIST(root='./data', train=True,  transform=data_transform, download=True)\n",
    "mnist_test  = datasets.MNIST(root='./data', train=False, transform=data_transform, download=True)\n",
    "\n",
    "# Prepare dataloaders\n",
    "train_dataloader = torch.utils.data.DataLoader(mnist_train, batch_size=64, shuffle=True)\n",
    "test_dataloader  = torch.utils.data.DataLoader(mnist_test,  batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "# Bring back the MNIST classfier model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1     = nn.Linear(28 * 28, 128)\n",
    "        self.relu    = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2     = nn.Linear(128, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x) # Flatten tensor\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.softmax(self.fc2(x))\n",
    "        return x\n",
    "    \n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create model\n",
    "model = NeuralNet().to(device)\n",
    "\n",
    "# Print summary\n",
    "summary(model, input_size=(1,28,28))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, optimizer):\n",
    "    # Iterate over #epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        # Keep track of network progress\n",
    "        train_loss    = 0.0\n",
    "        train_correct = 0\n",
    "        test_correct  = 0\n",
    "\n",
    "        # Visit each data sample once (random)\n",
    "        for image, labels in train_dataloader: \n",
    "            # Compute model prediction and loss\n",
    "            pred_labels = model(image.to(device))\n",
    "            loss        = CEloos(pred_labels, labels.to(device))\n",
    "\n",
    "            # Backpropagate\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()       \n",
    "\n",
    "            # Add loss to history\n",
    "            train_loss    += loss.item()\n",
    "            # Count number of correct predictions \n",
    "            train_correct += (torch.argmax(pred_labels.cpu(), 1) == labels.cpu()).sum().item()\n",
    "\n",
    "        # Test loop (once per epoch)\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_dataloader:\n",
    "                pred_labels = model(images.to(device))\n",
    "                test_correct += (torch.argmax(pred_labels.cpu(), 1) == labels.cpu()).sum().item()\n",
    "\n",
    "        # Compute accuracy (train & test)\n",
    "        train_acc = train_correct / len(mnist_train)\n",
    "        test_acc  = test_correct  / len(mnist_test)\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}, Train Acc: {:.2f}%, Test Acc: {:.2f}%'\n",
    "              .format(epoch, num_epochs, train_loss / len(mnist_train), 100 * train_acc, 100 * test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "CEloos   = nn.CrossEntropyLoss()\n",
    "optimizer_adam = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "train_model(num_epochs, optimizer_adam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two alternatives \n",
    "# Only saving the model\n",
    "torch.save(model.state_dict(), \"model/model.pt\")\n",
    "\n",
    "# Save model + optimizer + ... (training state)\n",
    "torch.save({'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_adam.state_dict(),\n",
    "            }, \"model/training_state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(\"model/training_state\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer_adam.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample and plot\n",
    "samples = next(iter(test_dataloader))[0][:5]\n",
    "labels  = next(iter(test_dataloader))[1][:5]\n",
    "\n",
    "p = [torch.argmax(k) for k in model(samples.to(device))]\n",
    "for i, l in enumerate(p):\n",
    "    plt.figure(figsize=(1,1))\n",
    "    plot_tensor(samples[i].squeeze(), labels[i].squeeze(), p[i].item())\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "beginner.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
