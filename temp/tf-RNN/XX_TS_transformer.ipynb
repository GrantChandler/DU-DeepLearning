{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "## Transformer Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T02:53:20.699018Z",
     "iopub.status.busy": "2022-12-14T02:53:20.698277Z",
     "iopub.status.idle": "2022-12-14T02:53:23.285398Z",
     "shell.execute_reply": "2022-12-14T02:53:23.284632Z"
    },
    "id": "7rZnJaGTWQw0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.10.0, Keras Vesion: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import IPython, IPython.display, os, datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks  import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Lambda, Dropout, Input\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "print(f\"Tensorflow Version {tf.__version__}, Keras Vesion: {keras.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "x = np.array([[[1.,2.,3.], [4.,5.,6.]] ])\n",
    "o = layers.MultiHeadAttention(key_dim=1, num_heads=2, dropout=0)(x, x, x)\n",
    "x.shape, o.shape, o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = {}\n",
    "models = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_84\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_87 (InputLayer)          [(None, 2, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " mha (MultiHeadAttention)       (None, 2, 2)         90          ['input_87[0][0]',               \n",
      "                                                                  'input_87[0][0]',               \n",
      "                                                                  'input_87[0][0]',               \n",
      "                                                                  'FeedForward[2][0]',            \n",
      "                                                                  'FeedForward[2][0]',            \n",
      "                                                                  'FeedForward[2][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization_51 (LayerN  (None, 2, 2)        4           ['mha[2][0]',                    \n",
      " ormalization)                                                    'mha[3][0]']                    \n",
      "                                                                                                  \n",
      " FeedForward (Sequential)       (None, 2, 2)         7           ['layer_normalization_51[2][0]', \n",
      "                                                                  'layer_normalization_51[3][0]'] \n",
      "                                                                                                  \n",
      " LastDecoderLyer (Sequential)   (None, 1, 10)        50          ['FeedForward[3][0]']            \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    input shape = windows_len, features_length  <== this is your input features dimentions\n",
    "    output shape= output_len , ouput_feat_len   <== This is your output features dimentions\n",
    "'''\n",
    "class TransformerModel():\n",
    "    def __init__(self, window_len, feat_len, output_len, output_feat_len, head_size=256, \n",
    "                num_heads=4, ff_dim=64, num_transformer_blocks=6, mlp_units=[128], \n",
    "                dropout=0.3, mlp_dropout=0.25, use_norm= \"layer|batch\", **kwargs ):\n",
    "        super().__init__()\n",
    "        input_shape = (window_len, feat_len)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_len, self.ouput_feat_len, self.head_size, self.num_heads, \\\n",
    "            self.ff_dim,self.num_transformer_blocks, self.mlp_units, self.dropout, self.mlp_dropout = \\\n",
    "                  output_len, ouput_feat_len, head_size, num_heads, ff_dim, \\\n",
    "                num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",
    "\n",
    "        #self.mha = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout, name=\"mha\")\n",
    "        self.mha = layers.MultiHeadAttention(key_dim=feat_len, num_heads=num_heads, dropout=dropout, name=\"mha\")\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "        self.ff = Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(feat_len),\n",
    "            layers.Dropout(mlp_dropout)\n",
    "            ], name=\"FeedForward\")\n",
    "\n",
    "        op_len = output_len * output_feat_len\n",
    "        \n",
    "        self.last = Sequential([layers.Flatten(), \n",
    "                                layers.Dropout(mlp_dropout), \n",
    "                                layers.Dense( op_len, activation=\"relu\"),\n",
    "                                layers.Reshape([output_len, output_feat_len])],\n",
    "                                name = \"LastDecoderLyer\")\n",
    "        self.build()\n",
    "\n",
    "    def position_encoding(self, x):\n",
    "        return x\n",
    "\n",
    "    def self_attention(self, inputs):\n",
    "        x = self.mha(inputs, inputs, inputs )\n",
    "        x = self.layernorm(x)\n",
    "        self.add( [x, inputs])\n",
    "        return x\n",
    "\n",
    "    def ff(self, inputs):\n",
    "        x = self.ff (inputs)\n",
    "        x = self.layernorm(x)\n",
    "        self.add( [inputs, x])\n",
    "        return x\n",
    "\n",
    "    def build(self, inputs =None):\n",
    "        if inputs is None:\n",
    "            inputs = keras.Input(shape=self.input_shape)\n",
    "        x = self.position_encoding(inputs)\n",
    "\n",
    "        for _ in range( self.num_transformer_blocks ):\n",
    "            x = self.self_attention(x)\n",
    "            x = self.ff(x)\n",
    "\n",
    "        x = self.last(x)\n",
    "        return keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "tm = TransformerModel(2, 2, output_len=1, output_feat_len=10, num_transformer_blocks =2, ff_dim=1)\n",
    "model = tm.build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_90\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_95 (InputLayer)          [(None, 2, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " mha (MultiHeadAttention)       (None, 2, 2)         90          ['input_95[0][0]',               \n",
      "                                                                  'input_95[0][0]',               \n",
      "                                                                  'input_95[0][0]']               \n",
      "                                                                                                  \n",
      " layer_normalization_56 (LayerN  (None, 2, 2)        4           ['mha[11][0]']                   \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " FeedForward (Sequential)       (None, 2, 2)         7           ['layer_normalization_56[11][0]']\n",
      "                                                                                                  \n",
      " LastDecoderLyer (Sequential)   (None, 1, 10)        50          ['FeedForward[11][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 151\n",
      "Trainable params: 151\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "\n",
    "'''\n",
    "    input shape = windows_len, features_length  <== this is your input features dimentions\n",
    "    output shape= output_len , ouput_feat_len   <== This is your output features dimentions\n",
    "'''\n",
    "class TransformerModel():\n",
    "    def __init__(self, window_len, feat_len, output_len, output_feat_len, head_size=256, \n",
    "                num_heads=4, ff_dim=64, num_transformer_blocks=6, mlp_units=[128], \n",
    "                dropout=0.3, mlp_dropout=0.25, use_norm= \"layer|batch\", **kwargs ):\n",
    "        super().__init__()\n",
    "        input_shape = (window_len, feat_len)\n",
    "        self.input_shape = input_shape\n",
    "        self.output_len, self.ouput_feat_len, self.head_size, self.num_heads, \\\n",
    "            self.ff_dim,self.num_transformer_blocks, self.mlp_units, self.dropout, self.mlp_dropout = \\\n",
    "                  output_len, ouput_feat_len, head_size, num_heads, ff_dim, \\\n",
    "                num_transformer_blocks, mlp_units, dropout, mlp_dropout\n",
    "\n",
    "        #self.mha = layers.MultiHeadAttention(key_dim=head_size, num_heads=num_heads, dropout=dropout, name=\"mha\")\n",
    "        self.mha = [layers.MultiHeadAttention(key_dim=feat_len, num_heads=num_heads, dropout=dropout, name=\"mha\")]*num_transformer_blocks\n",
    "        self.layernorm = layers.LayerNormalization()\n",
    "        self.add = layers.Add()\n",
    "\n",
    "        self.ff = Sequential([\n",
    "            layers.Dense(ff_dim, activation='relu'),\n",
    "            layers.Dense(feat_len),\n",
    "            layers.Dropout(mlp_dropout)\n",
    "            ] * num_transformer_blocks, name=\"FeedForward\")\n",
    "\n",
    "        op_len = output_len * output_feat_len\n",
    "        \n",
    "        self.last = Sequential([layers.Flatten(), \n",
    "                                layers.Dropout(mlp_dropout), \n",
    "                                layers.Dense( op_len, activation=\"relu\"),\n",
    "                                layers.Reshape([output_len, output_feat_len])],\n",
    "                                name = \"LastDecoderLyer\")\n",
    "        self.build()\n",
    "\n",
    "    def position_encoding(self, x):\n",
    "        return x\n",
    "\n",
    "    def self_attention(self, inputs):\n",
    "        x = self.mha(inputs, inputs, inputs )\n",
    "        x = self.layernorm(x)\n",
    "        self.add( [x, inputs])\n",
    "        return x\n",
    "\n",
    "    def ff(self, inputs):\n",
    "        x = self.ff (inputs)\n",
    "        x = self.layernorm(x)\n",
    "        self.add( [inputs, x])\n",
    "        return x\n",
    "\n",
    "    def build(self, inputs =None):\n",
    "        if inputs is None:\n",
    "            inputs = keras.Input(shape=self.input_shape)\n",
    "        x = self.position_encoding(inputs)\n",
    "\n",
    "        for i in range( self.num_transformer_blocks ):\n",
    "            x = self.mha[i](inputs, inputs, inputs )\n",
    "            x = self.layernorm(x)\n",
    "            self.add( [x, inputs])\n",
    "            x = self.ff(x)\n",
    "\n",
    "        x = self.last(x)\n",
    "        return keras.Model(inputs, x)\n",
    "\n",
    "\n",
    "tm = TransformerModel(2, 2, output_len=1, output_feat_len=10, num_transformer_blocks =6, ff_dim=1)\n",
    "model = tm.build()\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'dense_123/kernel:0' shape=(2, 1) dtype=float32, numpy=\n",
       " array([[0.59179795],\n",
       "        [0.12576783]], dtype=float32)>,\n",
       " <tf.Variable 'dense_123/bias:0' shape=(1,) dtype=float32, numpy=array([0.], dtype=float32)>,\n",
       " <tf.Variable 'dense_124/kernel:0' shape=(1, 2) dtype=float32, numpy=array([[0.39387882, 1.3983084 ]], dtype=float32)>,\n",
       " <tf.Variable 'dense_124/bias:0' shape=(2,) dtype=float32, numpy=array([0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm.ff.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6041/6041 [==============================] - 2684s 444ms/step - loss: 261953477935104.0000 - mean_absolute_error: 1880182.0000 - val_loss: 453047222272.0000 - val_mean_absolute_error: 164408.7812\n"
     ]
    }
   ],
   "source": [
    "history = ts_utils.compile_fit(model, window_trn, window_tst, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Model\n",
    "model1 = tf.keras.Sequential([\n",
    "    # Take the last time-step.\n",
    "    # Shape [batch, time, features] => [batch, 1, features]\n",
    "    tf.keras.layers.Lambda(lambda x: x[:, -1:, :])\n",
    "    ] + ts_utils.getCommonLayer(ouput_len, ouput_feat_len),\n",
    "    name = \"Linear\"\n",
    ")\n",
    "\n",
    "history1 = ts_utils.compile_fit(model1, window_trn, window_tst, epochs=1, verbose=1)\n",
    "\n",
    "models.append(model)x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hist = model.fit( window_trn, epochs=200, batch_size=batch_size, callbacks=callbacks)\n",
    "#model.evaluate(window_tst)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Evaluate All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performace={}\n",
    "models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, model in enumerate(models):\n",
    "    print(f\"Now Compiling {i+1}/{len(models)} {model.name} \")\n",
    "    history = ts_utils.compile_fit(model, window_trn, window_tst, epochs=3, verbose=1)\n",
    "    IPython.display.clear_output()\n",
    "\n",
    "# Plot graphs\n",
    "performance = ts_plot_utils.plot_performance(models, window_trn, window_tst, performance=performance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"PLotting {i} Model: {model.name}\")\n",
    "    d= plot_model(model, show_shapes=True)\n",
    "    display(d)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "model = models[0]\n",
    "rw=slice(400, 500)\n",
    "pc = 4\n",
    "\n",
    "for x in window_tst100:\n",
    "    #yp = model(x)\n",
    "    #yhat = yp[:, np.newaxis, :]\n",
    "    #y = x[1]\n",
    "    break;\n",
    "#y, yhat = ts_utils.model_predict( model , window_tst100)\n",
    "#ydf = scaler.inverse_transform(pd.DataFrame(y, columns=scaler.feature_names_in_[label_slice]))\n",
    "#pdf = scaler.inverse_transform(pd.DataFrame(yhat, columns=scaler.feature_names_in_[label_slice]))\n",
    "'''y = y.numpy().reshape((47754,20))\n",
    "yhat = yhat.numpy().reshape((47754,20))\n",
    "ydf = pd.DataFrame(y, columns=scaler.feature_names_in_[label_slice])\n",
    "pdf = pd.DataFrame(yhat, columns=scaler.feature_names_in_[label_slice])\n",
    "'''\n",
    "nc = len(ydf.columns)\n",
    "pr = nc //pc\n",
    "pr += 1 if len(ydf.columns)%pc else 0\n",
    "fig, axs = plt.subplots(pr, pc, figsize=(20, 3 *pc))\n",
    "for i, c in enumerate(ydf.columns):\n",
    "    pr1 , pc1 = i // pc, i % pc\n",
    "    plt1 = axs[ pr1, pc1 ]\n",
    "    plt1.plot(range(len(ydf))[rw], ydf[c][rw], \"-.\"  , label=f\"y\")\n",
    "    plt1.plot(range(len(pdf))[rw], pdf[c][rw], \"r-.\" , label=f\"yhat\")\n",
    "    plt1.legend()\n",
    "    plt1.set_title(f'{model.name} - {c} - {rw}/{len(ydf)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = models[1]\n",
    "#ydf, pdf = ts_plot_utils.predict_and_plot( model, window_trn100, window_tst100, howmany=1024* 1024,\n",
    "#                        plot_start=0, df=None, scaler=None, label_slice=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ts_anom_utils\n",
    "from ts_anom_utils import compute_scores\n",
    "\n",
    "y, yhat = ts_utils.model_predict( model , window_trn100)\n",
    "ydf = scaler.inverse_transform(pd.DataFrame(y, columns=scaler.feature_names_in_[label_slice]))\n",
    "pdf = scaler.inverse_transform(pd.DataFrame(yhat, columns=scaler.feature_names_in_[label_slice]))\n",
    "\n",
    "ret, error, se, errorDF, escaler, fscore = compute_scores(ydf, pdf, errorDF= None, escaler=None)\n",
    "\n",
    "y, yhat = ts_utils.model_predict( model , window_tst100)\n",
    "ydf = scaler.inverse_transform(pd.DataFrame(y, columns=scaler.feature_names_in_[label_slice]))\n",
    "pdf = scaler.inverse_transform(pd.DataFrame(yhat, columns=scaler.feature_names_in_[label_slice]))\n",
    "\n",
    "ret, error, se, errorDF, escaler, fscore = compute_scores(ydf, pdf, errorDF, escaler, fscore=fscore )\n",
    "\n",
    "errorDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.to_datetime(df_tst.index, unit='ms')[0:len(ret)], ret.norm_score)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The END"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ef3244febf2748b7299853ebe20fbefab2f9f94ffe70cfaeb9efc5b5372c95d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
