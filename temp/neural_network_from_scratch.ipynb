{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd112195",
   "metadata": {},
   "source": [
    "# Neural Network from scratch\n",
    "For this first assignment, you are asked to code a simple neural network from the scratch, using only numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4971c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Fix random seed \n",
    "np.random.seed(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9f8472",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dae4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    # Relu activation function \n",
    "    return np.maximum(0,x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    # Relu derivative function \n",
    "    x[x<=0] = 0\n",
    "    x[x>0]  = 1\n",
    "    return x\n",
    "    \n",
    "def sigmoid(x):\n",
    "    # Sigmoid activation function \n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def clip(x):\n",
    "    # Clipping threshold e\n",
    "    e = 0.0000000001\n",
    "    return np.maximum(x, e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d671aa",
   "metadata": {},
   "source": [
    "### Binary Cross Entropy Loss\n",
    "$L_{B C E}=-\\frac{1}{n} \\sum_{i=1}^n\\left(Y_i \\cdot \\log \\hat{Y}_i+\\left(1-Y_i\\right) \\cdot \\log \\left(1-\\hat{Y}_i\\right)\\right)$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04b8417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y, y_hat):   \n",
    "    ## Clipping (>0) to avoid NaNs in log \n",
    "    y_hat     = clip(y_hat) \n",
    "    n         = len(y) \n",
    "    l_bce     = -1/n * (np.sum(np.multiply(y, np.log(y_hat)) + np.multiply((1.0 - y), np.log(clip(1.0 - y_hat)))))\n",
    "    return l_bce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607401eb",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3951e448",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    # Calculates the accuracy between the predicted labels and the truth labels\n",
    "    # Fraction of predictions our model got right \n",
    "    acc = sum(y == y_hat) / len(y)\n",
    "    return acc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9bab7",
   "metadata": {},
   "source": [
    "### NN class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78000fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "        \n",
    "    def __init__(self, layers=[3,3,1]):\n",
    "        # Params dictionary holding weights and bias for our NN layers\n",
    "        self.parameters    = {}\n",
    "        self.layers        = layers     \n",
    "        \n",
    "        # Loss x iteration tracking\n",
    "        self.loss          = []\n",
    "        \n",
    "    def initialize_weights(self):\n",
    "        # Weights initialization using random normal distribution\n",
    "        # First layer\n",
    "        self.parameters[\"W1\"] = np.random.randn(self.layers[0], self.layers[1]) \n",
    "        self.parameters['b1'] = np.random.randn(self.layers[1])\n",
    "        # Second layer\n",
    "        self.parameters['W2'] = np.random.randn(self.layers[1],self.layers[2]) \n",
    "        self.parameters['b2'] = np.random.randn(self.layers[2])\n",
    "        \n",
    "    def fit(self, x, y, learning_rate=0.001, iterations=1000):\n",
    "        # Trains the neural network using the specified data (x) and labels (y)\n",
    "        # Reset weights and bias\n",
    "        self.initialize_weights() \n",
    "        \n",
    "        # Train a number of iterations\n",
    "        for i in range(iterations):\n",
    "            # Propagate x\n",
    "            y_hat = self.forward_propagation(x)\n",
    "            # Compute BCE loss\n",
    "            loss  = binary_cross_entropy_loss(y,y_hat)\n",
    "            self.loss.append(loss)\n",
    "            # Backpropagate\n",
    "            self.back_propagation(x,y,y_hat,learning_rate)\n",
    "    \n",
    "    def forward_propagation(self,x):\n",
    "        # Forward propagation\n",
    "        Z1 = x.dot(self.parameters['W1'])  + self.parameters['b1']\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = A1.dot(self.parameters['W2']) + self.parameters['b2']\n",
    "        y_hat = sigmoid(Z2)\n",
    "        \n",
    "        # save calculated parameters     \n",
    "        self.parameters['Z1'] = Z1\n",
    "        self.parameters['Z2'] = Z2\n",
    "        self.parameters['A1'] = A1\n",
    "\n",
    "        return y_hat\n",
    "    \n",
    "    def back_propagation(self,x,y,y_hat,learning_rate):\n",
    "        # Computes the derivatives, update weights and bias\n",
    "        y_inv     = 1 - y\n",
    "        y_hat_inv = 1 - y_hat\n",
    "\n",
    "        dl_wrt_y_hat = np.divide(y_inv, clip(y_hat_inv)) - np.divide(y, clip(y_hat))\n",
    "        dl_wrt_sig   = y_hat * (y_hat_inv)\n",
    "        dl_wrt_z2    = dl_wrt_y_hat * dl_wrt_sig\n",
    "\n",
    "        dl_wrt_A1    = dl_wrt_z2.dot(self.parameters['W2'].T)\n",
    "        dl_wrt_w2    = self.parameters['A1'].T.dot(dl_wrt_z2)\n",
    "        dl_wrt_b2    = np.sum(dl_wrt_z2, axis=0, keepdims=True)\n",
    "\n",
    "        dl_wrt_z1    = dl_wrt_A1 * relu_derivative(self.parameters['Z1'])\n",
    "        dl_wrt_w1    = x.T.dot(dl_wrt_z1)\n",
    "        dl_wrt_b1    = np.sum(dl_wrt_z1, axis=0, keepdims=True)\n",
    "\n",
    "        # Update the weights and bias\n",
    "        self.parameters['W1'] = self.parameters['W1'] - learning_rate * dl_wrt_w1\n",
    "        self.parameters['W2'] = self.parameters['W2'] - learning_rate * dl_wrt_w2\n",
    "        self.parameters['b1'] = self.parameters['b1'] - learning_rate * dl_wrt_b1\n",
    "        self.parameters['b2'] = self.parameters['b2'] - learning_rate * dl_wrt_b2\n",
    "        \n",
    "    def predict(self, x):\n",
    "        # Predicts on test data\n",
    "        Z1 = x.dot(self.parameters['W1'])  + self.parameters['b1']\n",
    "        A1 = relu(Z1)\n",
    "        Z2 = A1.dot(self.parameters['W2']) + self.parameters['b2']\n",
    "        pred = sigmoid(Z2)\n",
    "        return np.round(pred) \n",
    "\n",
    "    def plot_loss(self):\n",
    "        # Plots the loss curve\n",
    "        plt.plot(self.loss)\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Loss (BCE)\")\n",
    "        plt.title(\"Loss curve\")\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022a0f73",
   "metadata": {},
   "source": [
    "### Load dataset\n",
    "We will use the Habermanâ€™s Survival Dataset. The dataset contains cases from a study that was conducted between 1958 and 1970 at the University of Chicago's Billings Hospital on the survival of patients who had undergone surgery for breast cancer. There are 306 items (patients). There are three predictor variables (age, year of operation, number of detected nodes). The variable to predict is encoded as 1 (survived) or 2 (died). See https://archive.ics.uci.edu/ml/datasets/Haberman%27s+Survival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6d62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "headers =  ['age', 'year','nodes','y']\n",
    "haberman_df = pd.read_csv('haberman_data/haberman.data', sep=',', names=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde97bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "haberman_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a6e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas dataframe into numpy arrays\n",
    "x       = haberman_df.drop(columns=['y']).values[1:]\n",
    "y_label = haberman_df['y'].values[1:].reshape(x.shape[0], 1).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4ddc83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y_label, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the dataset\n",
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "from sklearn.preprocessing   import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "sc.fit(xtrain)\n",
    "xtrain = sc.transform(xtrain)\n",
    "xtest  = sc.transform(xtest)\n",
    "\n",
    "print(\"Shape of train set is {}\".format(xtrain.shape))\n",
    "print(\"Shape of test set is {}\".format(xtest.shape))\n",
    "print(\"Shape of train label is {}\".format(ytrain.shape))\n",
    "print(\"Shape of test labels is {}\".format(ytest.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409c8c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the Neural Network model\n",
    "nn = NeuralNetwork(layers=[3,5,1])\n",
    "nn.fit(xtrain, ytrain, learning_rate=0.0001, iterations=1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1bd120",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.plot_loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c05ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the training vs. testing data performance\n",
    "train_pred = nn.predict(xtrain)\n",
    "test_pred  = nn.predict(xtest)\n",
    "\n",
    "print(\"Train accuracy is {0:.2f}\".format(accuracy(ytrain, train_pred)))\n",
    "print(\"Test accuracy is {0:.2f}\".format(accuracy(ytest, test_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bd7243",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
