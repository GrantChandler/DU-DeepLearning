{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "### Anomaly Detection - Anomaly Scores  Calculations\n",
    "\n",
    "This notebook shows how to calculate anomaly scores for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T02:53:20.699018Z",
     "iopub.status.busy": "2022-12-14T02:53:20.698277Z",
     "iopub.status.idle": "2022-12-14T02:53:23.285398Z",
     "shell.execute_reply": "2022-12-14T02:53:23.284632Z"
    },
    "id": "7rZnJaGTWQw0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "2023-03-24 15:10:56.814754: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
=======
      "2023-03-05 21:15:13.365046: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
>>>>>>> cd6f3ce36c0d31b10d3c8e40e42bca839dbf57b0
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow Version 2.11.0, Keras Vesion: 2.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "error uploading: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x7fa564f3c640>, 'Connection to proxy-zsgov.external.lmco.com timed out. (connect timeout=15)'))\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "import IPython, IPython.display, os, datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "print(f\"Tensorflow Version {tf.__version__}, Keras Vesion: {keras.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Utility to compute anomaly score"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 5,
>>>>>>> cd6f3ce36c0d31b10d3c8e40e42bca839dbf57b0
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_anom_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_anom_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 06_anom_scores_calc.ipynb\n",
    "\n",
    "# Study this carefully\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    This will compute the F-score given y and yhat\n",
    "'''\n",
    "def computeFScore(y, yhat):\n",
    "    nu = np.sum( (y - yhat) ** 2)\n",
    "    de = 1 or np.sum( (y-np.average(y))**2 )\n",
    "\n",
    "    FSCORE=1-np.sqrt(nu/de)\n",
    "    return np.round(FSCORE, 4)\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    broken pairs columns are  \"x, y, resid, fitness, threshhold, ranking\"\n",
    "    This will return all sensors whose standardized error exceeds threshhold value\n",
    "'''\n",
    "def get_brknpairs(r, fscore, standardized_err, errdf, thr=1.4, topn=10):\n",
    "    r1  = round(r, 4)\n",
    "    se  =  np.round(abs(standardized_err.loc[r.name]), 4)\n",
    "    idx = abs(se.values).argsort()\n",
    "\n",
    "    ret = [[c, c, r1[c], fscore[c], thr, se[c]] for c in se[idx][::-1][:topn].index if abs(se[c]) > thr ]\n",
    "    top = [[c, se[c]] for c in se[idx][::-1][:topn].index]\n",
    "    return  len(ret), ret, top\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    compute Anomaly scores\n",
    "    ydf: original Y Values\n",
    "    pdf: predicted values\n",
    "\n",
    "    You must always pass escaler unless you are using validation data to get the estimate of the error\n",
    "\n",
    "    returns: \n",
    "        broken-pairs columns are  \"x, y, resid, fitness, threshhold, ranking\"\n",
    "'''\n",
    "def compute_scores(y: pd.DataFrame, yhat: pd.DataFrame, errorDF= None, escaler=None, top_n=15, fscore=None, file=None): \n",
    "    # 1. Compute the Absolute Error data frame\n",
    "    resid = y - yhat\n",
    "    error = abs(resid)\n",
    "\n",
    "    # 2. Lets scale if scaler is not given => assuming this is training errors\n",
    "    \n",
    "    if ( escaler is None ):\n",
    "        escaler = StandardScaler().fit(error)\n",
    "        standardized_err = pd.DataFrame(escaler.transform(error), columns=error.columns, index=resid.index)\n",
    "\n",
    "        # Compute the error statistics only if escaler is None\n",
    "        fscore = computeFScore(y, yhat)\n",
    "\n",
    "        errorDF = error.describe()\n",
    "        errorDF.loc['fscore'] = fscore\n",
    "        errorDF.loc['std_mean'] = standardized_err.mean()\n",
    "        errorDF.loc['std_std'] = standardized_err.std()\n",
    "    else:\n",
    "        standardized_err = pd.DataFrame(escaler.transform(error), columns=error.columns, index=resid.index)\n",
    "\n",
    "\n",
    "    # 4. Compute the Frobenius norm \n",
    "    score = np.linalg.norm(error.values ,axis=1)\n",
    "    norm_score = np.linalg.norm(standardized_err ,axis=1)\n",
    "\n",
    "    appl = resid.apply(get_brknpairs, args=( fscore, standardized_err, errorDF, 1.4, 20), axis=1)\n",
    "\n",
    "\n",
    "    # 6. put them all together\n",
    "    ret = pd.DataFrame(range(len(y)), columns=[\"line\"], index=y.index)\n",
    "    ret['time']             = y.index\n",
    "    ret['score']            = score\n",
    "    ret['norm_score']       = norm_score\n",
    "    ret['numBroken']        = [c[0] for c  in appl.values]\n",
    "    ret['brokenInvariants'] = [c[1] for c  in appl.values]\n",
    "    ret['brokenSensors']    = [c[2] for c  in appl.values]\n",
    "\n",
    "    if ( file is not None):\n",
    "        errorDF.to_csv(f'{file}_errordf.csv')\n",
    "        pickle.dump(escaler, open(f'{file}_escaler.pkl', 'wb'))\n",
    "\n",
    "    return ret, error, standardized_err, errorDF, escaler, fscore\n",
    "\n",
    "# The way how to use this \n",
    "'''\n",
    "    Split data into train, validation, and test data\n",
    "    Train on \"training\"\n",
    "    Compute yhat on validation data\n",
    "    call compute_scores on y and yhat of validation data.\n",
    "    Use the escaler to detect anomalies on test data\n",
    "    => COmpute the F-scrore, precision, recall scores\n",
    "''';\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the utility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%skip` not found.\n"
     ]
    }
   ],
   "source": [
    "%%skip echo \"\"\n",
    "file=\"/tmp/\"\n",
    "\n",
    "# Pass file parameter only when you want to save the parameters\n",
    "ret, error, standardized_err, errorDF, escaler, fscore = compute_scores(y, yhat, file=file)\n",
    "\n",
    "suffix = str(ret.index[0]).replace(\" \", '') + \"--\" + str(ret.index[-1]).replace(\" \", '')\n",
    "ret.to_csv(f'{file}SCORE-{suffix}.csv', index=False)\n",
    "\n",
    "\n",
    "# Read back and call for future calls\n",
    "file=\"/tmp/test_\"\n",
    "errorDF = pd.read_csv(f'{file}_errordf.csv', index_col=0)\n",
    "fscore = errorDF.loc['fscore']\n",
    "escaler= pickle.load(open(f'{file}_escaler.pkl', 'rb') )\n",
    "\n",
    "ret\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly score calculations explaination\n",
    "\n",
    "Explaination of the code above with an exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>&lt;-&gt;</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>&lt;-&gt;</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>&lt;-&gt;</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021-01-01 00:00:00</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>2.624345</td>\n",
       "      <td>1.388244</td>\n",
       "      <td>2.471828</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>-1.624345</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>0.528172</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>1.624345</td>\n",
       "      <td>0.611756</td>\n",
       "      <td>0.528172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 01:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>0.927031</td>\n",
       "      <td>3.865408</td>\n",
       "      <td>1.698461</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>1.072969</td>\n",
       "      <td>-0.865408</td>\n",
       "      <td>2.301539</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>1.072969</td>\n",
       "      <td>0.865408</td>\n",
       "      <td>2.301539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 02:00:00</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>4.744812</td>\n",
       "      <td>3.238793</td>\n",
       "      <td>5.319039</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>-1.744812</td>\n",
       "      <td>0.761207</td>\n",
       "      <td>-0.319039</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>1.744812</td>\n",
       "      <td>0.761207</td>\n",
       "      <td>0.319039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 03:00:00</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>1.750630</td>\n",
       "      <td>5.462108</td>\n",
       "      <td>3.939859</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>0.249370</td>\n",
       "      <td>-1.462108</td>\n",
       "      <td>2.060141</td>\n",
       "      <td>&lt;-&gt;</td>\n",
       "      <td>0.249370</td>\n",
       "      <td>1.462108</td>\n",
       "      <td>2.060141</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     A  B  C  <->         A         B         C  <->  \\\n",
       "2021-01-01 00:00:00  1  2  3  <->  2.624345  1.388244  2.471828  <->   \n",
       "2021-01-01 01:00:00  2  3  4  <->  0.927031  3.865408  1.698461  <->   \n",
       "2021-01-01 02:00:00  3  4  5  <->  4.744812  3.238793  5.319039  <->   \n",
       "2021-01-01 03:00:00  2  4  6  <->  1.750630  5.462108  3.939859  <->   \n",
       "\n",
       "                            A         B         C  <->         A         B  \\\n",
       "2021-01-01 00:00:00 -1.624345  0.611756  0.528172  <->  1.624345  0.611756   \n",
       "2021-01-01 01:00:00  1.072969 -0.865408  2.301539  <->  1.072969  0.865408   \n",
       "2021-01-01 02:00:00 -1.744812  0.761207 -0.319039  <->  1.744812  0.761207   \n",
       "2021-01-01 03:00:00  0.249370 -1.462108  2.060141  <->  0.249370  1.462108   \n",
       "\n",
       "                            C  \n",
       "2021-01-01 00:00:00  0.528172  \n",
       "2021-01-01 01:00:00  2.301539  \n",
       "2021-01-01 02:00:00  0.319039  \n",
       "2021-01-01 03:00:00  2.060141  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.511705</td>\n",
       "      <td>-0.238638</td>\n",
       "      <td>1.142703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.396298</td>\n",
       "      <td>1.097357</td>\n",
       "      <td>1.251523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.744812</td>\n",
       "      <td>-1.462108</td>\n",
       "      <td>-0.319039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.654462</td>\n",
       "      <td>-1.014583</td>\n",
       "      <td>0.316369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.687487</td>\n",
       "      <td>-0.126826</td>\n",
       "      <td>1.294156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.455270</td>\n",
       "      <td>0.649119</td>\n",
       "      <td>2.120490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.072969</td>\n",
       "      <td>0.761207</td>\n",
       "      <td>2.301539</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              A         B         C\n",
       "count  4.000000  4.000000  4.000000\n",
       "mean  -0.511705 -0.238638  1.142703\n",
       "std    1.396298  1.097357  1.251523\n",
       "min   -1.744812 -1.462108 -0.319039\n",
       "25%   -1.654462 -1.014583  0.316369\n",
       "50%   -0.687487 -0.126826  1.294156\n",
       "75%    0.455270  0.649119  2.120490\n",
       "max    1.072969  0.761207  2.301539"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets take a sample data and work through anomaly calculations\n",
    "# We create a \n",
    "'''\n",
    "    ydf => orignal values\n",
    "    pdf => predicted value\n",
    "    edf => error of the predictions\n",
    "'''\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "rw = np.array([1,2,3])\n",
    "ar = np.array([\n",
    "    rw, rw + 1, rw + 2, rw*2\n",
    "])\n",
    "ydf = pd.DataFrame(ar, columns=\"A B C\".split())\n",
    "ydf.index = pd.date_range(\"2021-1-1\", periods=len(ydf), freq=\"1H\")\n",
    "\n",
    "# Create a sample prediction data\n",
    "pdf = pd.DataFrame(ydf.values + np.random.normal(size=ydf.values.shape), columns=ydf.columns)\n",
    "pdf.index = ydf.index\n",
    "\n",
    "# Compute the Errr\n",
    "edf = ydf - pdf\n",
    "edf.index = ydf.index\n",
    "\n",
    "abs_edf = abs(edf)\n",
    "abs_edf.index = ydf.index\n",
    "\n",
    "sdf=pd.DataFrame(columns=[\"<->\"], data=['<->']*len(ydf))\n",
    "sdf.index = ydf.index\n",
    "display(pd.concat([ydf, sdf, pdf, sdf, edf, sdf, abs_edf],  axis=1))\n",
    "\n",
    "# This will describe your error distribution\n",
    "edf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764827</td>\n",
       "      <td>-0.970549</td>\n",
       "      <td>-0.873801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.169247</td>\n",
       "      <td>-0.184940</td>\n",
       "      <td>1.128095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.968906</td>\n",
       "      <td>-0.507671</td>\n",
       "      <td>-1.109884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1.564486</td>\n",
       "      <td>1.663160</td>\n",
       "      <td>0.855589</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          A         B         C\n",
       "0  0.764827 -0.970549 -0.873801\n",
       "1 -0.169247 -0.184940  1.128095\n",
       "2  0.968906 -0.507671 -1.109884\n",
       "3 -1.564486  1.663160  0.855589"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets compute the scale \n",
    "# you will save this errscale for future to validate test scores\n",
    "\n",
    "errscale = StandardScaler().fit(abs_edf)\n",
    "standardized_edf = pd.DataFrame( errscale.transform(abs_edf), columns=abs_edf.columns)\n",
    "standardized_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1, 2],\n",
       "        [2, 1],\n",
       "        [2, 0],\n",
       "        [1, 0]]),\n",
       " (4, 2),\n",
       " [array(['B', 'C'], dtype=object),\n",
       "  array(['C', 'B'], dtype=object),\n",
       "  array(['C', 'A'], dtype=object),\n",
       "  array(['B', 'A'], dtype=object)])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_n = 2   # How many top sensors you want -> if you have large data, you may want to choose 10\n",
    "suspicious_sensors_idx = np.argsort(-standardized_edf.abs().values )[:,:top_n]\n",
    "suspicious_sensors = [f for f in np.array(standardized_edf.columns)[suspicious_sensors_idx] ]\n",
    "\n",
    "suspicious_sensors_idx, suspicious_sensors_idx.shape, suspicious_sensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, 2])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this will give number of sensors deviated by more than by one SD\n",
    "threshold = 1.1\n",
    "\n",
    "vals = (standardized_edf.abs() > threshold)\n",
    "np.count_nonzero(vals, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.8143068 , 2.68277327, 1.93017874, 2.53852811]),\n",
       " array([1.51342428, 1.15561513, 1.55831656, 2.43839088]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = np.linalg.norm(abs_edf.values ,axis=1)\n",
    "norm_score = np.linalg.norm(standardized_edf.values ,axis=1)\n",
    "score, norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them to get a score you can write \n",
    "z = zip(score, norm_score, suspicious_sensors)\n",
    "adf=pd.DataFrame(z, columns=\"score norm_score suspicious_sensors\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally create scores file and optinally saVE IT\n",
    "scoreDF = pd.concat([adf, edf, standardized_edf], axis=1)\n",
    "#scoreDF.to_csv(\"/tmp/temp.csv\")\n",
    "#pd.read_csv(\"/tmp/temp.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorDF"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": null,
=======
   "execution_count": 1,
>>>>>>> cd6f3ce36c0d31b10d3c8e40e42bca839dbf57b0
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to call a URL and get results in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.8.3"
=======
   "version": "3.9.12"
>>>>>>> cd6f3ce36c0d31b10d3c8e40e42bca839dbf57b0
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
