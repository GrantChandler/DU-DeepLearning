{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "### Anomaly Detection - Anomaly Scores  Calculations\n",
    "\n",
    "This notebook shows how to calculate anomaly scores for anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-14T02:53:20.699018Z",
     "iopub.status.busy": "2022-12-14T02:53:20.698277Z",
     "iopub.status.idle": "2022-12-14T02:53:23.285398Z",
     "shell.execute_reply": "2022-12-14T02:53:23.284632Z"
    },
    "id": "7rZnJaGTWQw0"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import tensorflow\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.utils import plot_model\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "import IPython, IPython.display, os, datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "print(f\"Tensorflow Version {tf.__version__}, Keras Vesion: {keras.__version__}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Utility to compute anomaly score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile \"ts_anom_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 06_anom_scores_calc.ipynb\n",
    "\n",
    "# Study this carefully\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    This will compute the F-score given y and yhat\n",
    "'''\n",
    "def computeFScore(y, yhat):\n",
    "    nu = np.sum( (y - yhat) ** 2)\n",
    "    de = 1 or np.sum( (y-np.average(y))**2 )\n",
    "\n",
    "    FSCORE=1-np.sqrt(nu/de)\n",
    "    return np.round(FSCORE, 4)\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    broken pairs columns are  \"x, y, resid, fitness, threshhold, ranking\"\n",
    "    This will return all sensors whose standardized error exceeds threshhold value\n",
    "'''\n",
    "def get_brknpairs(r, fscore, standardized_err, errdf, thr=1.4, topn=10):\n",
    "    r1  = round(r, 4)\n",
    "    se  =  np.round(abs(standardized_err.loc[r.name]), 4)\n",
    "    idx = abs(se.values).argsort()\n",
    "\n",
    "    ret = [[c, c, r1[c], fscore[c], thr, se[c]] for c in se[idx][::-1][:topn].index if abs(se[c]) > thr ]\n",
    "    top = [[c, se[c]] for c in se[idx][::-1][:topn].index]\n",
    "    return  len(ret), ret, top\n",
    "\n",
    "'''~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "    compute Anomaly scores\n",
    "    ydf: original Y Values\n",
    "    pdf: predicted values\n",
    "\n",
    "    You must always pass escaler unless you are using validation data to get the estimate of the error\n",
    "\n",
    "    returns: \n",
    "        broken-pairs columns are  \"x, y, resid, fitness, threshhold, ranking\"\n",
    "'''\n",
    "def compute_scores(y: pd.DataFrame, yhat: pd.DataFrame, errorDF= None, escaler=None, top_n=15, file=None): \n",
    "    # 1. Compute the Absolute Error data frame\n",
    "    resid = y - yhat\n",
    "    error = abs(resid)\n",
    "\n",
    "    # 2. Lets scale if scaler is not given => assuming this is training errors\n",
    "    \n",
    "    if ( escaler is None ):\n",
    "        escaler = StandardScaler().fit(error)\n",
    "        standardized_err = pd.DataFrame(escaler.transform(error), columns=error.columns, index=resid.index)\n",
    "\n",
    "        # Compute the error statistics only if escaler is None\n",
    "        fscore = computeFScore(y, yhat)\n",
    "\n",
    "        errorDF = error.describe()\n",
    "        errorDF.loc['fscore'] = fscore\n",
    "        errorDF.loc['std_mean'] = standardized_err.mean()\n",
    "        errorDF.loc['std_std'] = standardized_err.std()\n",
    "    else:\n",
    "        standardized_err = pd.DataFrame(escaler.transform(error), columns=error.columns, index=resid.index)\n",
    "\n",
    "\n",
    "    # 4. Compute the Frobenius norm \n",
    "    score = np.linalg.norm(error.values ,axis=1)\n",
    "    norm_score = np.linalg.norm(standardized_err ,axis=1)\n",
    "\n",
    "    appl = resid.apply(get_brknpairs, args=( fscore, standardized_err, errdf, 1.4, 20), axis=1)\n",
    "\n",
    "\n",
    "    # 6. put them all together\n",
    "    ret = pd.DataFrame(range(len(y)), columns=[\"line\"], index=y.index)\n",
    "    ret['time']             = y.index\n",
    "    ret['score']            = score\n",
    "    ret['norm_score']       = norm_score\n",
    "    ret['numBroken']        = [c[0] for c  in appl.values]\n",
    "    ret['brokenInvariants'] = [c[1] for c  in appl.values]\n",
    "    ret['brokenSensors']    = [c[2] for c  in appl.values]\n",
    "\n",
    "    if ( file is not None):\n",
    "        errorDF.to_csv(f'{file}_errordf.csv')\n",
    "        pickle.dump(escaler, open(f'{file}_escaler.pkl', 'wb'))\n",
    "\n",
    "    return ret, error, standardized_err, errorDF, escaler, fscore\n",
    "\n",
    "# The way how to use this \n",
    "'''\n",
    "    Split data into train, validation, and test data\n",
    "    Train on \"training\"\n",
    "    Compute yhat on validation data\n",
    "    call compute_scores on y and yhat of validation data.\n",
    "    Use the escaler to detect anomalies on test data\n",
    "    => COmpute the F-scrore, precision, recall scores\n",
    "''';\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to use the utility "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=\"/tmp/\"\n",
    "\n",
    "# Pass file parameter only when you want to save the parameters\n",
    "ret, error, standardized_err, errorDF, escaler, fscore = compute_scores(y, yhat, file=file)\n",
    "\n",
    "suffix = str(ret.index[0]).replace(\" \", '') + \"--\" + str(ret.index[-1]).replace(\" \", '')\n",
    "ret.to_csv(f'{file}SCORE-{suffix}.csv', index=False)\n",
    "\n",
    "\n",
    "# Read back and call for future calls\n",
    "file=\"/tmp/test_\"\n",
    "errorDF = pd.read_csv(f'{file}_errordf.csv', index_col=0)\n",
    "fscore = errorDF.loc['fscore']\n",
    "escaler= pickle.load(open(f'{file}_escaler.pkl', 'rb') )\n",
    "\n",
    "ret\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly score calculations explaination\n",
    "\n",
    "Explaination of the code above with an exmaple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets take a sample data and work through anomaly calculations\n",
    "# We create a \n",
    "'''\n",
    "    ydf => orignal values\n",
    "    pdf => predicted value\n",
    "    edf => error of the predictions\n",
    "'''\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "rw = np.array([1,2,3])\n",
    "ar = np.array([\n",
    "    rw, rw + 1, rw + 2, rw*2\n",
    "])\n",
    "ydf = pd.DataFrame(ar, columns=\"A B C\".split())\n",
    "ydf.index = pd.date_range(\"2021-1-1\", periods=len(ydf), freq=\"1H\")\n",
    "\n",
    "# Create a sample prediction data\n",
    "pdf = pd.DataFrame(ydf.values + np.random.normal(size=ydf.values.shape), columns=ydf.columns)\n",
    "pdf.index = ydf.index\n",
    "\n",
    "# Compute the Errr\n",
    "edf = ydf - pdf\n",
    "edf.index = ydf.index\n",
    "\n",
    "abs_edf = abs(edf)\n",
    "abs_edf.index = ydf.index\n",
    "\n",
    "sdf=pd.DataFrame(columns=[\"<->\"], data=['<->']*len(ydf))\n",
    "sdf.index = ydf.index\n",
    "display(pd.concat([ydf, sdf, pdf, sdf, edf, sdf, abs_edf],  axis=1))\n",
    "\n",
    "# This will describe your error distribution\n",
    "edf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compute the scale \n",
    "# you will save this errscale for future to validate test scores\n",
    "\n",
    "errscale = StandardScaler().fit(abs_edf)\n",
    "standardized_edf = pd.DataFrame( errscale.transform(abs_edf), columns=abs_edf.columns)\n",
    "standardized_edf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n = 2   # How many top sensors you want -> if you have large data, you may want to choose 10\n",
    "suspicious_sensors_idx = np.argsort(-standardized_edf.abs().values )[:,:top_n]\n",
    "suspicious_sensors = [f for f in np.array(standardized_edf.columns)[suspicious_sensors_idx] ]\n",
    "\n",
    "suspicious_sensors_idx, suspicious_sensors_idx.shape, suspicious_sensors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will give number of sensors deviated by more than by one SD\n",
    "threshold = 1.1\n",
    "\n",
    "vals = (standardized_edf.abs() > threshold)\n",
    "np.count_nonzero(vals, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = np.linalg.norm(abs_edf.values ,axis=1)\n",
    "norm_score = np.linalg.norm(standardized_edf.values ,axis=1)\n",
    "score, norm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine them to get a score you can write \n",
    "z = zip(score, norm_score, suspicious_sensors)\n",
    "adf=pd.DataFrame(z, columns=\"score norm_score suspicious_sensors\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally create scores file and optinally saVE IT\n",
    "scoreDF = pd.concat([adf, edf, standardized_edf], axis=1)\n",
    "#scoreDF.to_csv(\"/tmp/temp.csv\")\n",
    "#pd.read_csv(\"/tmp/temp.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errorDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 (main, Apr  5 2022, 01:53:17) \n[Clang 12.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5ef3244febf2748b7299853ebe20fbefab2f9f94ffe70cfaeb9efc5b5372c95d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
