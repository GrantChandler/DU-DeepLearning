{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Utilities \n",
    "\n",
    "This will be included in all the files as generic utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/lstmutils1.py\n",
    "#!/usr/local/bin/python \n",
    "'''\n",
    "Some utility Functions to be used in all the apps\n",
    "#=*** NOTE *** DO NOT EDIT THIS FILE - THIS iS CREATED FROM: 01_utils.ipynb\n",
    "'''\n",
    "import re, sklearn, sys, os, datetime, glob, argparse, json, base64, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import preprocessing\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 4)\n",
    "mpl.rcParams['axes.grid'] = False\n",
    "\n",
    "pd.options.display.max_rows = 8\n",
    "\n",
    "sys.path.append(\"~/bin/gen\")\n",
    "import ccallbacks\n",
    "\n",
    "def getconfig(cf = \"config*\"):\n",
    "    confFiles = sorted(glob.glob(cf))\n",
    "    if ( len(confFiles) <= 0):\n",
    "        print(f\"No Configuration files {cf} found!!!\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Read and merge the configuration files\n",
    "    ret = {}\n",
    "    for cf in confFiles:\n",
    "        print(f\"#Getting Configuration from {cf}\")\n",
    "        with open(cf, \"r\") as f:\n",
    "            cf = f.read()\n",
    "\n",
    "        if (not cf.find('[START]') >=0 ):\n",
    "            r1 = cf\n",
    "        else:\n",
    "            r1=re.findall(\"\\[START](.*)\\[END]\", cf, flags=re.MULTILINE|re.DOTALL)\n",
    "            if ( len(r1) <= 0):\n",
    "                print(f\"Ignoring: Configuration not found in {cf}! no worries\")\n",
    "                continue\n",
    "            r1 = r1[0].replace(\"'\", '\"')    \n",
    "        rj = eval(r1)\n",
    "        ret.update(rj)\n",
    "                \n",
    "    return ret\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigList(conf, key=\"\"):\n",
    "    #print(f\"Getting {key}\")\n",
    "    ll = conf.get(key, []);\n",
    "    ret = []\n",
    "    for l in ll:\n",
    "        if type(l) == list:\n",
    "            ret += l\n",
    "        elif l.startswith(\"$\"):\n",
    "            ret += getConfigList(conf, l[1:])\n",
    "        else:\n",
    "            ret.append(l)\n",
    "\n",
    "    return ret\n",
    "#-----------------------------------------------------------------------------------\n",
    "def getConfigObject(conf, key=\"\"):\n",
    "    ll = conf.get(key, \"\");\n",
    "    if not ll:\n",
    "        return;\n",
    "    \n",
    "    sst = ll[0]\n",
    "    dec = base64.b64decode(sst)\n",
    "    ret = pickle.loads(dec, fix_imports=True)\n",
    "\n",
    "    return ret\n",
    "#--------------------------------------------------------------------------------\n",
    "def runMethod(pyMethod, **kwargs):\n",
    "    spl = pyMethod.split('.');\n",
    "\n",
    "    assert len(spl) >= 2, \"Hmmm ... May be not what is intended!! module name\"\n",
    "\n",
    "    modName = \".\".join(spl[:-1])\n",
    "    __import__(modName, fromlist=\"dummy\")\n",
    "\n",
    "    funName = spl[-1]\n",
    "    ret = None\n",
    "    for v in sys.modules:\n",
    "        if (v.startswith(modName)):\n",
    "            method= getattr(sys.modules[v], funName)\n",
    "            print(v, type(v), funName, method, type(method), callable(method))\n",
    "            ret = method(**kwargs)\n",
    "    return ret\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def getInvertedPreds(conf, yh):\n",
    "    scalerY = utils1.getConfigObject(conf, \"scalerYString\")\n",
    "    sOuputs = utils1.getConfigList(conf, 'scaleOutputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    \n",
    "    yhdf    = pd.DataFrame(yh, columns=ouputs)    # Dataframe of Predictions\n",
    "    ys      = yhdf[sOuputs].values                # Values to be scaledback\n",
    "    yi      = scalerY.inverse_transform(ys)       # inverse transform the outputs\n",
    "    yidf    = yhdf.copy()                         # Copy and set the values\n",
    "    yidf[sOuputs] = yi\n",
    "    return yhdf, yidf\n",
    "    \n",
    "#--------------------------------------------------------------------------------\n",
    "def getOriginal(conf, unnormdf, index=0):\n",
    "    inputs  = utils1.getConfigList(conf, 'inputs')\n",
    "    ouputs  = utils1.getConfigList(conf, 'outputs')\n",
    "    tsParams= conf.get(\"tsParams\", {})\n",
    "    \n",
    "    index   = 0\n",
    "    startIX = index + tsParams['length']\n",
    "    batchSZ = 1 # batch size\n",
    "    stride  = tsParams.get('stride', 1)\n",
    "    i = startIX + batchSZ * stride * index\n",
    "    return unnormdf[i:], inputs, ouputs\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plotInverted(conf, yh, unnormdf, s =-400, howmany=100):\n",
    "    e=s+howmany\n",
    "    \n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "\n",
    "    x = pd.to_datetime(yorg[yorg.columns[0]][s:e])\n",
    "    \n",
    "    plt.plot(x, yorg[ops].values[s:e], marker='.', label=\"Original\")\n",
    "    plt.plot(x, yidf.values[s:e], marker='x', label=\"Predicted\")\n",
    "    plt.title(\"Plotting Inverted Values:\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "\n",
    "'''\n",
    "Reconstruct the original diffed columns\n",
    "'''    \n",
    "def reconstructOrig(conf, unnormdf, yh, ouputs):\n",
    "    yhdf, yidf = getInvertedPreds(conf, yh)\n",
    "    yorg,ips,ops = getOriginal(conf, unnormdf)\n",
    "    \n",
    "    for o in ouputs:\n",
    "        if(o.endswith(\"___diff1\")):\n",
    "            oc = o[:-8]\n",
    "            print(f\"Getting Original column for: '{oc}' \")\n",
    "            if ( oc not in yorg.columns):\n",
    "                print('Cannot compute the orginal column values from diffs for {oc}')\n",
    "                continue;\n",
    "                \n",
    "            ## WOW <== this is heavy - undo the diffing in the opposite way\n",
    "            yidf[oc] = yorg[oc].values + yidf[o].shift(-1)\n",
    "\n",
    "    return yidf # y inverted dataframe with adjusted cols for diffs\n",
    "\n",
    "\n",
    "\n",
    "#runMethod(\"gen.utils1.is_number\", **{\"s\":\"123.78\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/plotutils1.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (16, 3)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "sys.path.append(\".\")\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import lstmfit;\n",
    "import sklearn.metrics\n",
    "\n",
    "def predict(modelFile, valg, model=None):\n",
    "    m1 = model or load_model(modelFile)\n",
    "    #xxt = np.array([valg[i][0][0] for i in range(len(valg))])\n",
    "    #yyt = np.array([valg[i][1][0] for i in range(len(valg))])\n",
    "    yh=m1.predict(valg)\n",
    "    return yh\n",
    "            \n",
    "def plot1(modelFile, valg, model=None, idx=0, n=-150, howmany=50):\n",
    "    yh = predict(modelFile, valg, model)\n",
    "    yy = np.array([valg[i][1][idx] for i in range(len(valg))])\n",
    "                 \n",
    "    plt.gcf().set_size_inches(22, 10, forward=True)\n",
    "    plt.plot( yy[n:n+howmany], marker='o', label=\"original-\")\n",
    "    plt.plot( yh[n:n+howmany], marker='x', label=\"predicted\")\n",
    "    mse = sklearn.metrics.mean_squared_error(yy, yh)\n",
    "    \n",
    "    plt.title(f\"{modelFile} : {model}: MSE: {mse} <==\")\n",
    "    plt.grid()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return yy, yh, mse\n",
    "\n",
    "\n",
    "def plotyyyh(columns, xxx, yy1, yh1, s, e):\n",
    "    for i in range(yh1.shape[-1]):\n",
    "        yyy = yy1[:,i]\n",
    "        yyh = yh1[:,i]\n",
    "\n",
    "        r2 = sklearn.metrics.r2_score(yyy, yyh)\n",
    "        if (r2 <0.5):\n",
    "            print(f'--{i}: {columns[i+1]} R^2: {r2}')\n",
    "            continue\n",
    "\n",
    "        plt.title(f'{i}: {columns[i+1]} R^2: {r2}')\n",
    "\n",
    "        plt.plot(xxx[s:e], yyy[s:e], marker='.', label=\"original\")\n",
    "        plt.plot(xxx[s:e], yyh[s:e], marker='x', label=\"predictd\")\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    \n",
    "def plotstuff(valg, confile, normeddf=None, model = None, mcpoint=None ):\n",
    "    if ( model is None):\n",
    "        conf, unnormdf, normeddf, inps, oups = lstmfit.getConf(confile)\n",
    "        model, mcpoint = lstmfit.getModel(conf)\n",
    "\n",
    "    print(len(valg), mcpoint.best)\n",
    "    \n",
    "    xxx = pd.to_datetime(normeddf.time[-len(valg):])\n",
    "    mcpoint.drawLosses()\n",
    "\n",
    "    yh1 = model.predict(valg)            \n",
    "    yy1 = np.array([valg[j][1][0] for j in range(len(valg))])\n",
    "\n",
    "    s = 0\n",
    "    e = s + len(valg)\n",
    "    plotyyyh(normeddf.columns, xxx, yy1, yh1, s, e)\n",
    "    \n",
    "    return model, mcpoint, normeddf, yy1, yh1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Configure  - dataprep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /opt/utils/gen/dataprep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile  /opt/utils/gen/dataprep.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM: sada/NNBook/notebooks/NNetworks/LSTM/01_Utils.ipynb\n",
    "\n",
    "import re, sys, os, datetime, getopt, glob, argparse, datetime, json, base64, pickle,colabexts\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "#--------------------------------------------------------------------------------\n",
    "def runMethod(pyMethod, **kwargs):\n",
    "    spl = pyMethod.split('.');\n",
    "\n",
    "    assert len(spl) >= 2, \"Hmmm ... May be not what is intended!! module name\"\n",
    "\n",
    "    modName = \".\".join(spl[:-1])\n",
    "    __import__(modName, fromlist=\"dummy\")\n",
    "\n",
    "    funName = spl[-1]\n",
    "    ret = None\n",
    "    for v in sys.modules:\n",
    "        if (v.startswith(modName)):\n",
    "            method= getattr(sys.modules[v], funName)\n",
    "            print(v, type(v), funName, method, type(method), callable(method))\n",
    "            ret = method(**kwargs)\n",
    "    return ret\n",
    "\n",
    "#------ 1.--------------------------------------------------------------------------\n",
    "def dfselect(df, columns=[], **kwargs):\n",
    "    if not columns or len(columns) <= 0:\n",
    "        return df\n",
    "    df1 = df[columns]\n",
    "    \n",
    "    return df1\n",
    "#----- 2.---------------------------------------------------------------------------\n",
    "def drop_nonnumerics(df, **kwargs):\n",
    "    #cols = df.columns[df.dtypes.eq('object')]\n",
    "    cols = df.select_dtypes(exclude=np.number).columns\n",
    "    cols = [c for c in cols if not c.lower().startswith('time')]\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** Non numeric columns => {cols}: Dropping them\")\n",
    "        df.drop(cols, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: no nonnumeric columns\")\n",
    "        \n",
    "    return df\n",
    "#----- 3.---------------------------------------------------------------------------\n",
    "# Drop any columns with less than 6 unique values\n",
    "#\n",
    "def drop_unique(df, unique=6, **kwargs):\n",
    "    unique_vals  = df.nunique()\n",
    "    cols         = unique_vals[ unique_vals <= unique].index\n",
    "\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: *** dropping columns having <= {unique} values => {cols}\")\n",
    "        df.drop(cols, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: Nothing to drop\")\n",
    "        \n",
    "    return df\n",
    "    \n",
    "#----- 4a.---------------------------------------------------------------------------\n",
    "# Find any sensor highly correlated with time and drop them.\n",
    "def detectTimeCorrelated(df, timecol=\"time\", val=0.94, **kwargs):\n",
    "    timecol = df.columns[0]\n",
    "    \n",
    "    timeser = pd.Series(df[[timecol]].values.reshape(-1))\n",
    "    if ( timeser.dtype != np.number ):\n",
    "        timeser = pd.to_datetime(timeser).astype(int)\n",
    "    \n",
    "    \n",
    "    DROP_INDEX = 0; # Debugging\n",
    "    corcols    = []\n",
    "    for sensor in df.columns:\n",
    "        if (sensor == timecol ):\n",
    "            continue;\n",
    "        #print(f\"#Testing {sensor}...\")\n",
    "        # The following code tries to detect correlation by dropping first 8 or last 8 values\n",
    "        # sometimes dropping first few will show correlation due to start up times\n",
    "        sensorSeries = pd.Series(df[sensor].values.reshape(-1))\n",
    "        for i in range(8):\n",
    "            c1 = timeser[i:].corr(sensorSeries[i:])\n",
    "            c2 = timeser[i:].corr(sensorSeries[:-i])\n",
    "            if np.abs(c1) >= val or np.abs(c2) >= val:\n",
    "                corcols.append(sensor)\n",
    "                DROP_INDEX = max(DROP_INDEX, i) #lets drop first few rows\n",
    "                break;\n",
    "                \n",
    "    #print(f\"#Time Cor: #{len(timeCorSensors)}, #Shape before:{df.shape}\")\n",
    "    #df.drop(timeCorSensors, axis=1, inplace=True)\n",
    "    #df = df[DROP_INDEX:]\n",
    "    #print(f\"#After dropping: {DROP_INDEX} =>{df.shape}\")\n",
    "        \n",
    "    return corcols\n",
    "#----- 3.---------------------------------------------------------------------------\n",
    "# Drop any time correlated sensors\n",
    "#\n",
    "def drop_time_correlated(df, timecol=\"time\", corr=0.95, **kwargs):\n",
    "    cols = detectTimeCorrelated(df, timecol, corr)\n",
    "\n",
    "    if (len(cols) > 0):\n",
    "        print(f\"WARNING: Dropping time correlated columns having corr-coeffient >= {corr} => {cols}\")\n",
    "        df.drop(cols, axis=1, inplace=True, errors=\"ignore\")\n",
    "    else:\n",
    "        print(f\"+Good: No time correlated columns having corr-coeffient >= {corr}\")\n",
    "        \n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Covert to one_hot encoding with prefix for columns'\n",
    "def make_OneHot(df, oheCols=[], **kwargs):\n",
    "    ohe = pd.DataFrame();\n",
    "    for c in oheCols:\n",
    "        one_hot = pd.get_dummies(df[c])\n",
    "        nc = [f'{c}___{k}' for k in one_hot.columns]\n",
    "        one_hot.columns = nc\n",
    "        ohe = pd.concat([ohe, one_hot], axis=1)\n",
    "\n",
    "    return ohe\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Assuming the tf1 is sorted in ascending order of time\n",
    "def add_Diff(tf1, col=[], **kwargs):\n",
    "    if (type(col) == str):\n",
    "        col = [col]\n",
    "    for c in col:\n",
    "        if ( c not in tf1.columns):\n",
    "            print(f\"*WARNING* Column {c} Not FOUND\")\n",
    "            continue\n",
    "        print(f\"+++ Adding {c}\")\n",
    "        tf1[f'{c}___diff1'] = tf1[c] - tf1[c].shift(1)\n",
    "    return tf1\n",
    "#-----------------------------------------------------------------------------------\n",
    "# Assuming the tf1 is sorted in ascending order of time\n",
    "# column1 = 'MSFT_close'\n",
    "# column2 = 'AAPL_close'\n",
    "def add_corr(df, column1, column2, window  = 100, stride=1, **kwargs):\n",
    "    c1      = df[column1]\n",
    "    c2      = df[column2]\n",
    "    corr    = []\n",
    "    for i in range(0, len(c1) - window +1, stride):\n",
    "        cc1 = c1[i : i+window]\n",
    "        cc2 = c2[i : i+window]\n",
    "        cor = cc1.corr( cc2 )\n",
    "        corr.append(cor)\n",
    "    df = df[window:]    \n",
    "    df[column1+\"___\"+column2] = corr\n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "def _convertObjToStr(obj):\n",
    "    astr = base64.b64encode(pickle.dumps(obj, protocol=None, fix_imports=True))\n",
    "    astr = astr.decode(\"utf-8\")\n",
    "    return astr\n",
    "def _convertStrToObj(astr):\n",
    "    astr = base64.b64decode(astr)\n",
    "    obj  = pickle.loads(decoded,fix_imports=True)\n",
    "    return obj\n",
    "#-----------------------------------------------------------------------------------\n",
    "def add_movingavg(df, cols = [], window=0, dropna=True, **kwargs):\n",
    "    for c in cols:\n",
    "        a = df[c].rolling(window=window).mean()\n",
    "        df[f'{c}__MOVING_AVG'] = a\n",
    "    if (dropna):\n",
    "        df.dropna(inplace=True)\n",
    "    #uni_data.plot(subplots=True)\n",
    "    #u3.plot(subplots=True, color='red')\n",
    "    return df;\n",
    "\n",
    "def add_expmovingavg(df, cols = [], window=0, dropna=True, **kwargs):\n",
    "    for c in cols:\n",
    "        a = df[c].ewm(span=window,adjust=False).mean()\n",
    "        df[f'{c}__EXP_MOVING_AVG'] = a\n",
    "    if (dropna):\n",
    "        df.dropna(inplace=True)\n",
    "        \n",
    "    return df\n",
    "\n",
    "def data_fill(df,  **kwargs):\n",
    "    df = df.ffill().bfill()\n",
    "    df.dropna(inplace=True)\n",
    "        \n",
    "    return df\n",
    "#-----------------------------------------------------------------------------------\n",
    "# This will scale the numeric columns - if this changes - you need to use dataprep\n",
    "#    \"scaler\"         : [\"sklearn.preprocessing.StandardScaler()\"],\n",
    "#    \"scaler\"         : [\"sklearn.preprocessing.MinMaxScaler()\"],\n",
    "def data_scale(df, cols = [], start=0, pct=0.9, count=0, scaler=StandardScaler, **kwargs):\n",
    "    if ( type(scaler) == type ):\n",
    "        scaleri = scaler()\n",
    "    elif ( type(scaler) == str ):\n",
    "        scaleri = _convertStrToObj(scaler)\n",
    "    else:\n",
    "        scaleri = scaler\n",
    "    \n",
    "    \n",
    "    if isinstance(df, pd.DataFrame):\n",
    "        cols = cols or [c for c in df.columns] \n",
    "        df1  = df[cols]\n",
    "    else:\n",
    "        df1  = df\n",
    "        \n",
    "    tCnt = count or int(len(df) * pct) or None\n",
    "        \n",
    "    sx1  = scaleri.fit(df1[start:tCnt])        # Fit only training part \n",
    "    di1  = scaleri.transform(df1)              # Tranform the entire df\n",
    "    dfi  = pd.DataFrame(di1, columns=cols )\n",
    "\n",
    "    return dfi, _convertObjToStr(scaleri);\n",
    "#-----------------------------------------------------------------------------------\n",
    "def process():\n",
    "    print(\"Dont know what to do!! NOW\")\n",
    "    \n",
    "#-----------------------------------------------------------------------------------\n",
    "sysargs=None\n",
    "def addargs():\n",
    "    global sysargs\n",
    "    p = argparse.ArgumentParser(f\"{os.path.basename(sys.argv[0])}:\")\n",
    "    p.add_argument('-c', '--config', type=str, default=\"config.txt\", help=\"Config Files\")\n",
    "    p.add_argument('-o', '--output', type=str, default=0, help=\"output file\")\n",
    "    p.add_argument('args', nargs=argparse.REMAINDER)\n",
    "    p.add_argument('input_files',action=\"store\", type=str, nargs='+', help=\"input file(s)\")\n",
    "\n",
    "    try:\n",
    "        sysargs, unknown=p.parse_known_args(sys.argv[1:])\n",
    "    except argparse.ArgumentError as exc:\n",
    "        print(exc.message )\n",
    "        \n",
    "    if (unknown):\n",
    "        print(\"Unknown options: \", unknown)\n",
    "        #p.print_help()\n",
    "    return sysargs    \n",
    "#-----------------------------------------------------------------------------------\n",
    "if __name__ == '__main__':\n",
    "    if (not colabexts.jcommon.inJupyter()):\n",
    "        t1 = datetime.datetime.now()\n",
    "        sysargs = addargs()\n",
    "        ret = process()\n",
    "        t2 = datetime.datetime.now()\n",
    "        print(f\"#All Done in {str(t2-t1)} ***\")\n",
    "    else:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 1,  2,  3],\n",
       "        [11, 21, 33],\n",
       "        [12, 22, 32],\n",
       "        [13, 23, 33]]),\n",
       " array([[ 1,  2],\n",
       "        [11, 21],\n",
       "        [12, 22],\n",
       "        [13, 23]]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=[\n",
    "    [1,2,3],\n",
    "    [11,21,33],\n",
    "    [12,22,32],\n",
    "    [13,23,33],\n",
    "]\n",
    "a1=np.array(a)\n",
    "a1, a1[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3, 33, 32, 33])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1[:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/stockdata_ext.csv'\n",
    "f='/opt/SCHAS/NNBook/notebooks/NNetworks/LSTM/data/daily_MSFT.csv'\n",
    "conf, dfUnNormalized, dfNormalized = process('config.*', [f], \"out.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Callback for Saving and Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/ccallbacks.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import re, sys, os, datetime, glob, json, base64, pickle, sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import Callback\n",
    "import IPython\n",
    "from IPython.display import display\n",
    "\n",
    "sys.path.append(\"gen\")\n",
    "sys.path.append(\"/opt/LMCO/git/bin/gen\")\n",
    "import lstmutils1;\n",
    "import sklearn.metrics\n",
    "\n",
    "class ModelCheckAndLoad(Callback):\n",
    "    def __init__(self, filepath, monitor='val_loss', best=np.inf, \n",
    "                 stop_at=False, verbose=0, drawLoss=False):\n",
    "        super(Callback, self).__init__()\n",
    "        self.monitor  = monitor\n",
    "        self.filepath = filepath\n",
    "        self.verbose  = verbose\n",
    "        self.best     = best or np.inf\n",
    "        self.stop_at  = stop_at;\n",
    "        self.history  = {}\n",
    "        self.epochs   = []\n",
    "        self.drawLoss = drawLoss\n",
    "        self.epochNum = 0\n",
    "        self.numSaved = 0\n",
    "        \n",
    "    def save_ext(self):\n",
    "        ef = self.filepath+\"_ext\"\n",
    "        with open(ef, \"wb\") as f:\n",
    "            myParams = {\n",
    "                'best'     : self.best,\n",
    "                'bestEpoch': self.bestEpoch,\n",
    "                'epochNum' : self.epochNum,\n",
    "                'history'  : self.history,\n",
    "                'monitor'  : self.monitor\n",
    "            }\n",
    "            pickle.dump(myParams, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "                \n",
    "    def save_latest(self):\n",
    "        self.model.save(self.filepath+\"_latest\", overwrite=True)\n",
    "        self.save_ext();\n",
    "            \n",
    "    def load_ext(self):\n",
    "        ret = None;\n",
    "        if ( os.path.exists(self.filepath+\"_latest\")):\n",
    "            ret = load_model(self.filepath+\"_latest\")\n",
    "            print(\"Loading from the latest:...\")\n",
    "        elif ( os.path.exists(self.filepath)):\n",
    "            ret = load_model(self.filepath)\n",
    "        \n",
    "        ef = self.filepath+\"_ext\"\n",
    "        if ( not os.path.exists(ef) or os.path.getsize(ef) <= 0):\n",
    "            return ret\n",
    "        \n",
    "        with open(ef, \"rb\") as f:\n",
    "            myParams      = pickle.load(f)\n",
    "            self.best     = myParams.get('best'    , np.inf)\n",
    "            self.epochNum = myParams.get('epochNum', 0);\n",
    "            self.history  = myParams.get('history', {});\n",
    "            self.monitor  = myParams.get('monitor'  , \"val_loss\");\n",
    "            \n",
    "        print(f\"Best Loaded {self.best} occured at: {self.epochNum}\")\n",
    "        return ret;\n",
    "\n",
    "    def drawLosses(self):\n",
    "        history, best = self.history, self.best\n",
    "        #IPython.display.clear_output(wait=True)\n",
    "        plt.clf()\n",
    "\n",
    "        fig, ax1 = plt.subplots()\n",
    "        i, colors, marks = 0, \"rgbcmykw\", \"v.xo+\"\n",
    "\n",
    "        color = colors[i]\n",
    "        ax1.set_xlabel('epochs')\n",
    "        k, v = \"loss\", history['loss']\n",
    "        ax1.set_ylabel(k, color=color)\n",
    "        l1= ax1.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "        i +=1\n",
    "        k, v = \"val_loss\", history['val_loss']\n",
    "        color = colors[i]\n",
    "        ax2.set_ylabel(k, color=color)\n",
    "        l2 = ax2.plot(v, color=color, marker=marks[i], label=f\"{k}\")\n",
    "\n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        l3 = plt.plot(0, best, marker=\"o\",  c=\"b\", label=f\"BEST: {best}\")\n",
    "        ax1.grid()\n",
    "\n",
    "        lns  = l1 + l2 + l3;\n",
    "        labs = [l.get_label() for l in lns]\n",
    "        plt.legend(lns, labs, loc=0)\n",
    "        plt.show()\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.epochs.append(epoch)\n",
    "        self.epochNum += 1;\n",
    "        \n",
    "        for k, v in logs.items():\n",
    "            self.history.setdefault(k, []).append(v)\n",
    "            \n",
    "        self.current = logs.get(self.monitor)\n",
    "        if self.current is None:\n",
    "            warnings.warn(f'Can save best model only with {self.monitor} available')\n",
    "            return;\n",
    "                    \n",
    "        if (self.best > self.current):\n",
    "            ou= f'{self.monitor}: {self.best} > {self.current}\\n'\n",
    "            print(f\"Epoch: {epoch+1} Saving: {ou}\");\n",
    "            \n",
    "            self.numSaved += 1\n",
    "            self.bestEpoch+= 1\n",
    "            self.best      = self.current\n",
    "            self.model.save(self.filepath, overwrite=True)\n",
    "            self.save_ext();\n",
    "            self.model.stop_training = self.stop_at\n",
    "        elif self.verbose > 0:\n",
    "            ou= f'{self.monitor}: {self.best} <= {self.current}'\n",
    "            print(f\"{epoch+1} din't improve : {ou} from {self.bestEpoch}\\r\", end=\"\")\n",
    "            \n",
    "        if (self.drawLoss):\n",
    "            drawLosses(self.history, self.best)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definition of Some Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile  ~/bin/gen/LSTMModelDefs.py\n",
    "#!/usr/local/bin/python \n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, LSTM, Input, Activation, concatenate\n",
    "from keras.layers import RepeatVector, TimeDistributed\n",
    "from keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "def SimpleModel1(history, nfeatures, nOut, **kwargs) :\n",
    "    lstm_input = Input(shape=(history, nfeatures), name='lstm_input')\n",
    "    x = LSTM(50, name='lstm_0')(lstm_input)\n",
    "    x = Dropout(0.2, name='lstm_dropout_0')(x)\n",
    "    x = Dense(64, name='dense_0')(x)\n",
    "    x = Activation('sigmoid', name='sigmoid_0')(x)\n",
    "    x = Dense(nOut, name='dense_1')(x)\n",
    "    output = Activation('linear', name='linear_output')(x)\n",
    "\n",
    "    model = Model(inputs=lstm_input, outputs=output)\n",
    "    adam = optimizers.Adam(lr=0.0005)\n",
    "    model.compile(optimizer=adam, loss='mse')\n",
    "    \n",
    "    return model\n",
    "\n",
    "def SimpleModel2(inps, inshape, units2=None, nsteps=1, opt=\"adam\", loss=\"mse\", bi=False, dropout=None):\n",
    "    s= inshape\n",
    "    print(locals())\n",
    "    print(f\"Creating LSTM: inuts= {inps} time-steps: {s[0]}, features: {s[1]} #out: {nsteps}\")\n",
    "    m = keras.models.Sequential()\n",
    "\n",
    "    if (bi):\n",
    "        m.add(keras.layers.Bidirectional(\n",
    "            keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) ) )\n",
    "    else:\n",
    "        m.add(keras.layers.LSTM(inps, return_sequences= (units2 is not None), input_shape=s) )\n",
    "    \n",
    "    if(units2 is not None): #Lets just keep it simple for 2 layers only\n",
    "        m.add(keras.layers.LSTM(units2, activation='relu'))\n",
    "    if (dropout is not None):\n",
    "        m.add( keras.layers.Dropout(dropout) )\n",
    "    m.add(keras.layers.Dense(nsteps))\n",
    "    m.compile(optimizer = opt, loss= loss)\n",
    "    return m\n",
    "\n",
    "def UberModel(lookBack, nFeatures, lstm_IPDim=256, lstm_OPDim=1, opt=None, loss=\"rmse\",  drop=0.3):\n",
    "    opt        = opt or optimizers.Adam(lr=0.0005)\n",
    "    k_rrizer   = None\n",
    "    r_rrizer   = None\n",
    "\n",
    "    input_layer  = Input(shape=(lookBack, nFeatures), dtype='float32', name='input')\n",
    "    memory_layer = LSTM( lstm_IPDim, return_sequences=True, name=\"memory1\")(input_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=False, name=\"memory2\")(memory_layer)\n",
    "    repeated     = RepeatVector(lookBack)(memory_layer)\n",
    "    memory_layer = LSTM (int(lstm_IPDim/2), return_sequences=True, name=\"first1out\")(repeated)\n",
    "    memory_layer = LSTM (lstm_IPDim,  return_sequences=True, name=\"first2out\")(memory_layer)\n",
    "    decoded_inputs = TimeDistributed(Dense(units=lstm_OPDim, activation='linear'))( memory_layer)\n",
    "\n",
    "    #  Try spatial dropout?\n",
    "    dropout_input = Dropout(drop)(input_layer)\n",
    "    concat_layer  = concatenate([dropout_input, decoded_inputs])\n",
    "\n",
    "    memory_layer = LSTM (units=lstm_IPDim, \n",
    "                             kernel_regularizer = k_rrizer, \n",
    "                             recurrent_regularizer = r_rrizer, \n",
    "                             return_sequences=False)(concat_layer)\n",
    "    preds = Dense(units=lstm_OPDim, activation='linear')(memory_layer)\n",
    "\n",
    "    model1 = Model(input_layer, preds)\n",
    "    model1.compile(optimizer = opt, loss= loss)             \n",
    "\n",
    "    return model1"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "294.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
