{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XVhK72Pu1cJL"
   },
   "source": [
    "## Imports and Setup - Data prep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (14, 4)\n",
    "mpl.rcParams['axes.grid'] = True\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    Load the data you prepared - you must have run 01_ts_dataprep \n",
    "'''\n",
    "def load_file( file = '../data/jena_climate_2009_2016.csv.zip'):\n",
    "    df = pd.read_csv(file+\".csv\")\n",
    "    df['Date Time'] = pd.to_datetime( df['Date Time'], format='%Y-%m-%d %H:%M:%S' )\n",
    "    df_scaled_trn   = pd.read_csv(file+\".trn.csv\")\n",
    "    df_scaled_tst   = pd.read_csv(file+\".tst.csv\")\n",
    "    scaler          = pickle.load(open(f'{file}.scaler.pkl', 'rb'))\n",
    "\n",
    "    # You can inverse transform predicted value to get original value \n",
    "    # pd.DataFrame(scaler.inverse_transform(scaler.transform(df_train)))\n",
    "\n",
    "    return df, df_scaled_trn, df_scaled_tst, scaler\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    dataset:        must be tf.data.Dataset.from_tensor_slices\n",
    "    label_slice:    labels (indices or slice(start,end, skip) )\n",
    "    window_len:     Length of the window\n",
    "    output_len:     Length of the labels (# of steps to predict)\n",
    "\n",
    "Usage:\n",
    "    df = pd.read_csv(file) or [[0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3]]\n",
    "    ds = timeseries_dataset_from_dataset(df, 2, 2, slice(0, 2))\n",
    "    #print_dataset(ds)\n",
    "\n",
    "'''\n",
    "def window(dataset, window_len, output_length, label_slice=slice(0,1), batch_size=1, skip = 0 ):\n",
    "    ds = dataset.window(window_len + skip + output_length, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda x: x).batch(window_len + skip+ output_length)\n",
    "     \n",
    "    def split_feature_label(x):\n",
    "        return x[:window_len], x[window_len+skip:,label_slice]\n",
    "     \n",
    "    ds = ds.map(split_feature_label)\n",
    "\n",
    "    return ds.batch(batch_size)\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Compute the Average of the training output and we will use this as default predictions\n",
    "# Also for computing R-squared value\n",
    "def compute_avg(window):\n",
    "    count, total = 0, None;\n",
    "    for w in window:\n",
    "        if (not count):\n",
    "            total = w[1]\n",
    "        else:\n",
    "            total += w[1]\n",
    "        count += 1\n",
    "\n",
    "    avg_output = total/count\n",
    "    return avg_output\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "'''\n",
    "    predict the model,\n",
    "    y:      is the original array of expected \n",
    "    yhat:   is the predicted values\n",
    "'''\n",
    "def model_predict(model, window, y=None, yhat= None, howmany=1024*1024):\n",
    "    for w in window.take(howmany):\n",
    "        xc = w[0]\n",
    "        yc = w[1]\n",
    "        yp = model.predict(xc, verbose=0)\n",
    "\n",
    "        yc = yc[:,-1,:]\n",
    "        yp = yp[:,-1,:]\n",
    "\n",
    "        if ( y is None):\n",
    "            y = yc\n",
    "            yhat = yp\n",
    "            continue;\n",
    "        \n",
    "        y = np.concatenate([y,yc])\n",
    "        yhat = np.concatenate([yhat,yp])\n",
    "\n",
    "    return y, yhat\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "# Define inv_transform functions - Note: yh: [batch, time, features length]\n",
    "def inverse_transform(yh, scaler, label_slice, df=None):\n",
    "    yy=np.empty([yh.shape[0], scaler.n_features_in_])\n",
    "    yy[:] = np.nan\n",
    "\n",
    "    yy[:, label_slice] = yh\n",
    "    ys = scaler.inverse_transform(yy)\n",
    "\n",
    "    if (df is not None):\n",
    "        ys = pd.DataFrame(ys[:, label_slice], columns=df.columns[label_slice])\n",
    "\n",
    "    return ys    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ts_plot_utils.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile \"ts_plot_utils.py\"\n",
    "\n",
    "# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pickle\n",
    "import ts_utils\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plot(y, yh, x=None, title=\"\", scaler=None):\n",
    "    if (scaler):\n",
    "        y1  = scaler.inverse_transform(y) \n",
    "        yh1 = scaler.inverse_transform(yh) \n",
    "    else:\n",
    "        y1, yh1 = y, yh\n",
    "\n",
    "    x = x or range(max(len(y1),len(yh)))\n",
    "    \n",
    "    plt.scatter(x, y1,  marker='.', s=64, edgecolor='k', label=\"Y\")\n",
    "    plt.scatter(x, yh1, marker='x', s=64, edgecolor='k', label=\"$\\hat{y}$\")\n",
    "    plt.title(title)\n",
    "    plt.grid(1)\n",
    "    plt.legend()\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plotFeatureImportance(weights, labels):\n",
    "    plt.bar( x = range(len(weights)), height=weights)\n",
    "    if (labels):\n",
    "        print(labels)\n",
    "        axis = plt.gca()\n",
    "        axis.set_xticks(range(len(labels)))\n",
    "        axis.tick_params(axis='both', which='major', labelsize=15)\n",
    "\n",
    "        _ = axis.set_xticklabels(labels, rotation=90)\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def eval_performance(model, trn_dataset, tst_dataset=None, metric_name=\"loss\"):\n",
    "    en = model.evaluate(trn_dataset)\n",
    "    if (tst_dataset):\n",
    "        et = model.evaluate(tst_dataset);\n",
    "    else:\n",
    "        et = [0] * len(en)\n",
    "\n",
    "    mi = max(0, model.metrics_names.index(metric_name))\n",
    "\n",
    "    return np.array(en).flat[mi], np.array(et).flat[mi]\n",
    "\n",
    "performance = {}\n",
    "def plot_performance(models, trn_dataset, tst_dataset=None, metric_name=\"loss\", performance = {}, reeval=0):\n",
    "    for m in models:\n",
    "        if (not reeval and performance.get(m.name, None)):\n",
    "            print(f\"Performance for {m.name} exists\")\n",
    "            continue;  # Dont evaluate if performance is already computed\n",
    "\n",
    "        performance[m.name] = eval_performance(m, trn_dataset, tst_dataset, metric_name)\n",
    "\n",
    "    if (len(performance) <= 0 ):\n",
    "        print(\"No models to plot?\")\n",
    "\n",
    "    x = np.arange(len(performance))\n",
    "    width = 0.3\n",
    "    val_mae =  [v[0] for v in performance.values()]\n",
    "    test_mae = [v[1] for v in performance.values()]\n",
    "\n",
    "    plt.title(f\"Comparisons of '{metric_name}' : \")\n",
    "    plt.ylabel('Metrics')\n",
    "    plt.bar(x - 0.17, val_mae, width,  label= f'Training {metric_name}')\n",
    "    plt.bar(x + 0.17, test_mae, width, label= f'Test {metric_name}')\n",
    "    plt.xticks(ticks=x, labels=performance.keys(), rotation=45)\n",
    "    _ = plt.legend()\n",
    "    \n",
    "    return performance\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def plot_predictions(ydf, yhatdf, start=0, end=1024*1024, title=\"\"):\n",
    "    plt.figure(figsize=(14, 4))\n",
    "\n",
    "    for c in ydf.columns:\n",
    "        y1, p1 = ydf[c][start:end], yhatdf[c][start:end]\n",
    "        plt.scatter( y1.index, y1, edgecolors='k', marker='o', label= f'{c}: y',    c='#2ca02c' )\n",
    "        plt.scatter( p1.index, p1, edgecolors='k', marker='X', label= f'{c}: yhat', c='#ff7f0e')\n",
    "\n",
    "        plt.title = title\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------\n",
    "def predict_and_plot( model, window_trn, window_tst, howmany=1024* 1024,\n",
    "                        plot_start=0, plot_end=1024*1024, df=None, scaler=None, label_slice=None):\n",
    "    y, yhat = None, None\n",
    "    y, yhat = ts_utils.model_predict( model , window_trn,  y, yhat, howmany)\n",
    "    if (window_tst is not None):\n",
    "        y, yhat = ts_utils.model_predict( model , window_tst,  y, yhat, howmany)\n",
    "\n",
    "    if ( df is not None):\n",
    "        ydf = ts_utils.inverse_transform(y, scaler, label_slice, df)\n",
    "        pdf = ts_utils.inverse_transform(yhat, scaler, label_slice, df)\n",
    "    else:\n",
    "        ydf = pd.DataFrame(y   )\n",
    "        pdf = pd.DataFrame(yhat)\n",
    "\n",
    "    plot_predictions(ydf,pdf, plot_start, plot_end, title=f\"{model.name}\")\n",
    "\n",
    "    return ydf, pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/snarayan/venv/py39/lib/python3.8/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "[[[ 0.91819914 -2.01847258 -2.07559971 -1.95168275]\n",
      "  [ 0.93257019 -2.11680321 -2.17421487 -2.0966161 ]\n",
      "  [ 0.95891712 -2.10851026 -2.16834492 -2.0805124 ]]]\n",
      "=>:\n",
      "[[[-2.20565618 -2.27048205]\n",
      "  [-2.21039501 -2.27752599]]] \n",
      "\n",
      "[[[ 0.93257019 -2.11680321 -2.17421487 -2.0966161 ]\n",
      "  [ 0.95891712 -2.10851026 -2.16834492 -2.0805124 ]\n",
      "  [ 0.97688093 -2.13694321 -2.19769467 -2.13321544]]]\n",
      "=>:\n",
      "[[[-2.21039501 -2.27752599]\n",
      "  [-2.15115969 -2.22352245]]] \n",
      "\n",
      "[[[ 0.95891712 -2.10851026 -2.16834492 -2.0805124 ]\n",
      "  [ 0.97688093 -2.13694321 -2.19769467 -2.13321544]\n",
      "  [ 1.03316755 -2.20565618 -2.27048205 -2.22544575]]]\n",
      "=>:\n",
      "[[[-2.15115969 -2.22352245]\n",
      "  [-2.02439611 -2.10612345]]] \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 13:30:51.495701: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Test window function\n",
    "\n",
    "ds = tf.data.Dataset.from_tensor_slices(df_scaled_trn[df_scaled_trn.columns[:4]])\n",
    "wd = window(ds, 3, 2, slice(1,3), 1,1)\n",
    "for w in wd.take(3):\n",
    "    print(f\"{w[0].numpy()}\\n=>:\\n{w[1].numpy()} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "time_series.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3 (default, Jul  2 2020, 11:26:31) \n[Clang 10.0.0 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "dff0aaeeb8ee9738611fdcb903e0426fbcf38bc2d039ac205716a81cc1909598"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
