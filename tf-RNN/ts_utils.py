
# DO NOT EDIT THIS FILE - GENERATED FROM 02_ts_utils.ipynb

import tensorflow as tf
from tensorflow import keras
from keras.callbacks  import EarlyStopping

import matplotlib as mpl
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, MinMaxScaler
import pickle

mpl.rcParams['figure.figsize'] = (14, 4)
mpl.rcParams['axes.grid'] = True

#--------------------------------------------------------------------------------
'''
    Load the data you prepared - you must have run 01_ts_dataprep 
'''
def load_file( file = '../data/jena_climate_2009_2016.csv.zip'):
    df = pd.read_csv(file+".csv")
    df['Date Time'] = pd.to_datetime( df['Date Time'], format='%Y-%m-%d %H:%M:%S' )
    df_scaled_trn   = pd.read_csv(file+".trn.csv")
    df_scaled_tst   = pd.read_csv(file+".tst.csv")
    scaler          = pickle.load(open(f'{file}.scaler.pkl', 'rb'))

    return df, df_scaled_trn, df_scaled_tst, scaler

#--------------------------------------------------------------------------------
'''
    dataset:        must be tf.data.Dataset.from_tensor_slices
    label_slice:    labels (indices or slice(start,end, skip) )
    window_len:     Length of the window
    output_len:     Length of the labels (# of steps to predict)

    for_aencoder:   Note: for auto encoder, output is same as input

Usage:
    df = pd.read_csv(file) or [[0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3], [0,1,2,3]]
    ds = timeseries_dataset_from_dataset(df, 2, 2, slice(0, 2))
    #print_dataset(ds)

'''
def window(dataset, window_len, output_length, label_slice=slice(0,1), batch_size=1, skip = 0):
    ds = dataset.window(window_len + skip + output_length, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda x: x).batch(window_len + skip+ output_length)
     
    def split_feature_label(x):
        return x[:window_len], x[window_len+skip:,label_slice]
     
    ds = ds.map(split_feature_label)
    return ds.batch(batch_size)

def windowae(dataset, window_len, batch_size=1):
    ds = dataset.window(window_len, shift=1, drop_remainder=True)
    ds = ds.flat_map(lambda x: x).batch(window_len)
     
    def split_feature_label(x):
        return x, x
        #return x[:window_len], x[:window_len]
     
    ds = ds.map(split_feature_label)
    return ds.batch(batch_size)

#--------------------------------------------------------------------------------
# Compute the Average of the training output and we will use this as default predictions
# Also for computing R-squared value
def compute_avg(window):
    count, total = 0, None;
    for w in window:
        if (not count):
            total = w[1]
        else:
            total += w[1]
        count += 1

    avg_output = total/count
    return avg_output

#--------------------------------------------------------------------------------
'''
    predict the model,
    y:      is the original array of expected 
    yhat:   is the predicted values
'''
def model_predict(model, window, y=None, yhat= None, howmany=1024*1024):
    for w in window.take(howmany):
        xc = w[0]
        yc = w[1]
        yp = model.predict(xc, verbose=0)

        yc = yc[:,-1,:]
        yp = yp[:,-1,:]

        if ( y is None):
            y = yc
            yhat = yp
            continue;
        
        y = np.concatenate([y,yc])
        yhat = np.concatenate([yhat,yp])

    return y, yhat


#--------------------------------------------------------------------------------
# Define inv_transform functions - Note: yh: [batch, time, features length]
def inverse_transform(yh, scaler, label_slice, df=None):
    yy=np.empty([yh.shape[0], scaler.n_features_in_])
    yy[:] = np.nan

    yy[:, label_slice] = yh
    ys = scaler.inverse_transform(yy)

    if (df is not None):
        ys = pd.DataFrame(ys[:, label_slice], columns=df.columns[label_slice])

    return ys    

#--------------------------------------------------------------------------------
def compile_fit(model, window_trn=None, window_tst= None, opt=None, patience=3, epochs=1, callbacks=[], **kwargs):
    earlyStopCB = EarlyStopping(monitor='val_loss', patience=patience, mode='min', restore_best_weights = True)

    callbacks.append(earlyStopCB)

    # You may use these callbacks to save the model if you wanted to

    #saveModelCB = ModelCheckpoint(filepath=model.name + ".model.tf", save_best_only=True, verbose=0)
    #tensorBrdCB = TensorBoard(log_dir= f'./logs/{model.name}', histogram_freq=0, write_graph=True, write_images=True)

    loss = tf.keras.losses.MeanSquaredError()
    opt  = opt or tf.keras.optimizers.Adam()
    mets = [tf.keras.metrics.MeanAbsoluteError()]

    ##=> Other options you can try
    #learning_rate = 1e-6
    #opt = tf.keras.optimizers.SGD(learning_rate=learning_rate, momentum=0.9)
    #opt = tf.keras.optimizers.SGD()
    #loss=tf.keras.losses.Huber()

    model.compile(loss= loss, optimizer= opt, metrics=mets)

    history = []
    if (window_trn is not None):
        history = model.fit(window_trn, epochs=epochs, validation_data=window_tst, 
                                workers=4, use_multiprocessing=True, callbacks=callbacks, **kwargs)

    return history

#--------------------------------------------------------------------------------
# This commonLayer, a layer that is common to all models
def getCommonLayer(ouput_len, ouput_feat_len, previousLayer=None):
    op_len = ouput_len * ouput_feat_len;
    commonLayer = [
        # Shape => [batch, 1, out_len * #features]
        tf.keras.layers.Dense( op_len, activation='linear', kernel_initializer=tf.initializers.zeros()),
        
        # Shape => [batch, out_steps, features]
        tf.keras.layers.Reshape([ouput_len, ouput_feat_len])
    ]
    if (previousLayer is not None):
        preds = commonLayer[0](previousLayer)
        preds = commonLayer[1](preds)
        commonLayer = preds

    return commonLayer
